# Python Backend for Audio Classification
This folder contains the Python backend that was used for the implementation and training of two different models.
License, Author etc.


## Installation and Run Instructions
All requirements are stored in the requirements.txt file. 
In order to install them, please run the command ```pip install```.

To create the models, that can be used by the Android App, please run the following scripts:

1. ```TrainModel.py```
2. ```ConvertModelToTFLite.py```
3. ```WriteMetadataToModel.py```

After running all three scripts, copy the tensorflow lite model that contains the metadata to your Android App.

## Script Explanation
This section gives a short introduction to each script.
All scripts can be run without arguments and the parameters can be changed directly in the scripts.

### Constants.py
This file contains all global constants that are used by multiple scripts such as the sample rate of audio tracks or the length of audio pieces used by the model.
In addition, it contains helper methods.
Constants can be changed but must always be the same as in the front end to get meaningful results.

### labels.txt
This file contains all labels that should be predicted by the model.
It is used for the generation of meta data.
The labels must be ordered according to the folder structure containing the data.
This means, that the first label in this file is the name of the first data folder (e.g. "silence").

### TrainModel.py
This file contains the two models that are trained on the data sets.
The models are trained sequentially, with the same data set.
Training parameters and the used data can be changed directly in this file.

The first model, a 1D convolutional neuronal network, was taken and adapted from: 
It works with the raw audio input (the spectrogram) from the .wav files of length 32000 (since we use a sample rate of 16000 Hz and an audio track length of 2 seconds).

The second model, a 2D convolution neuronal network, was taken from: 
It works with mel spectrograms that are generated by the librosa library.
The length of audio pieces and the sample rate is equal as in the first model.
The parameters for the mel spectrogram creation can be changed in this file directly but they must be equal to the parameters in the front end to get meaningful results.

Both models are saved for further processing.

### ConvertModelToTFLite.py
This file contains a script that transforms a tensorflow model (.pb) to a tensorflow lite (.tflite) model.
The lite models are needed by the front end.

Tne code is taken from:
### WriteMetadataToModel.py
This script writes meta data to the tensorflow lite model since this is required by the front end.

The code is taken from:
### PlotSpectrogram.py
This script plots a sample spectrogram for visual inspection.
The sample spectrum is a one dimensional array taken from ```SampleSpectrogram.py```.

### SampleSpectrogram.py
This file contains a sample spectrogram that can be plotted in another script.

### TestSpecModel.py
This script takes one audio track as input and predicts the labels for it with the one dimensional Spectrogram model.
If an audio track is longer that the track length that was used for the model creation, it is cut into smaller pieces of the required length.
For all pieces, the prediction is then run.

The prediction function is called with the raw audio input from the .wav file without further preprocessing.
The script handles the input processing and cutting of audio tracks by itself.

### TestMelSpecModel.py
This script takes one audio track as input and predicts the labels for it with the two dimensional Mel Spectrogram model.
If an audio track is longer that the track length that was used for the model creation, it is cut into smaller pieces of the required length.

For all pieces, a mel spectrogram is computed by the librosa library.
This mel spectrogram is then used for running the model prediction.
The model runs the prediction for each audio piece.
The script handles the input processing, cutting of audio tracks, and computation of mel spectrograms by itself.

## Data 
The data for the training of the model was taken from the following resources:

Speech:


Song:

Silence: Privately recorded with the tool Audacity.

Since the audio data was not always in the right form, pre-processing has been applied to the data with the tool Audacity.
The audio data was converted from stereo to mono, converted to a sample rate of 16000 Hz and for the song and speech data sets, silence was manually removed (e.g. at the beginning of an audio track).
In addition, several speech audio samples were combined into one track since not all of them were long enough.
This resulted in several speech audio files that consist of multiple speech samples (in a serial form) from different people.

The data is organised in two folders: data_short and data_long.
The data_long folder contains all available pre-processed tracks and is used for the actual training.
The data_short folder contains only a small subset of all available pre-processed data.
IT is mainly used for testing the whole pipeline since it requires less time.

## References and Resources
This list contains all resources that were used for the project:

* [`Tensorflow Metadata Writer Information`](https://www.tensorflow.org/lite/convert/metadata)
* [`Tensorflow Metadata Converter (Code)`](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/metadata_writer_tutorial.ipynb)
* [`Tensorflow Lite Model Converter`](https://www.tensorflow.org/lite/convert)
* [`1D CNN Model for Spectrogram Data`](https://github.com/Logan97117/environmental_sound_classification_1DCNN)

add librosa, tensorflow lite, numpy, matplot lib etc everything that was used.
Audacity
